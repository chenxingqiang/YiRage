# YiRage - Yield Revolutionary AGile Engine

<div align="center">

# YiRage: Multi-Backend LLM Inference Optimization

**Yield Revolutionary AGile Engine**

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.8+-green.svg)](https://www.python.org/)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)]()

**Based on [Mirage](https://github.com/mirage-project/mirage) by CMU**

</div>

---

## üéØ About YiRage

**YiRage** (Yield Revolutionary AGile Engine) is a compiler and runtime system that extends the foundational work of Mirage (CMU) with comprehensive **multi-backend support**, enabling LLM inference optimization across diverse hardware platforms.

### YiRage = Mirage + Multi-Backend Architecture

- **Original Mirage** (CMU, 2023-2024): Superoptimizer framework
- **YiRage Extensions** (Chen Xingqiang, 2025): Multi-backend support

---

## ‚ú® YiRage Enhancements

### üöÄ 7 Complete Backend Implementations

| Backend | Hardware | Status | Key Features |
|---------|----------|--------|--------------|
| **CUDA** | NVIDIA GPU | ‚úÖ | Tensor Core, Warp optimization, Bank conflict avoidance |
| **CPU** | x86/ARM | ‚úÖ | SIMD (AVX512/AVX2), Cache blocking, OpenMP |
| **MPS** | Apple Silicon | ‚úÖ | Metal shaders, Unified memory, GPU family detection |
| **Triton** | Compiler | ‚úÖ | Auto-tuning, Software pipelining, Split-K |
| **NKI** | AWS Neuron | ‚úÖ | SBUF optimization, DMA scheduling, BF16 support |
| **cuDNN** | CUDA Lib | ‚úÖ | Algorithm selection, Tensor Op, Workspace optimization |
| **MKL** | Intel Lib | ‚úÖ | Threading modes, MKL BLAS, Fast matrix multiply |

### üéØ Hardware-Aware Kernel Optimizers

- **42+ Optimization Methods** across all backends
- **Automatic Configuration** based on hardware capabilities
- **Performance Modeling** for each backend

#### Example: CUDA Optimizer
```python
from yirage.kernel.cuda import CUDAOptimizer, CUDAKernelConfig

config = CUDAKernelConfig()
CUDAOptimizer.optimize_grid_block_dims(1024, 1024, 1024, compute_capability=80, config=config)

# Auto-configured:
# - Tensor Core: 16x8x16 (Ampere)
# - Warps: 16
# - Shared memory: Swizzled layout (no bank conflicts)
# - Occupancy: >75%
```

### üîç Backend-Specific Search Strategies

- **5 Independent Search Strategies**
- **15 Candidate Generation Dimensions**
- **13 Performance Evaluation Metrics**

#### Example: CUDA Search Strategy
```python
from yirage.search import SearchStrategyFactory, SearchConfig

strategy = SearchStrategyFactory.create_strategy(type.BT_CUDA, SearchConfig())
best_config = strategy.optimize(graph)

# Evaluates:
# - Occupancy (30%)
# - Memory efficiency (30%)
# - Compute throughput (30%)
# - Bank conflicts (10%)
```

### üìä Code Statistics

```
Backend Layer:      1,900 lines
Kernel Optimizers:  2,380 lines
Search Strategies:  2,220 lines
Documentation:      5,700 lines
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:             16,900+ lines
```

---

## üöÄ Quick Start

### Installation

```bash
git clone --recursive https://github.com/chenxingqiang/yirage
cd yirage
pip install -e . -v
export YIRAGE_HOME=$(pwd)
```

### Basic Usage

```python
import yirage as yr

# Query available backends
backends = yr.get_available_backends()
print(f"Available backends: {backends}")
# Output: ['cuda', 'cpu', 'mps', 'triton']

# Check backend availability
if yr.is_backend_available('cuda'):
    print("CUDA ready!")

# Get backend information
info = yr.get_backend_info('cuda')
print(f"CUDA devices: {info.get('device_count', 0)}")

# Create PersistentKernel with specific backend
ypk = yr.PersistentKernel(
    mode="decode",
    backend="cuda",              # Specify backend
    fallback_backends=["cpu"],   # Fallback if CUDA unavailable
    world_size=1,
    mpi_rank=0,
    # ... other parameters
)

# Compile and run
ypk.compile()
ypk()
```

### Using Hardware-Specific Optimizers

```python
# CUDA optimization
from yirage.kernel.cuda import CUDAOptimizer, CUDAKernelConfig

cuda_config = CUDAKernelConfig()
CUDAOptimizer.optimize_grid_block_dims(m=1024, n=1024, k=1024, 
                                        compute_capability=80, 
                                        config=cuda_config)

# CPU optimization  
from yirage.kernel.cpu import CPUOptimizer, CPUKernelConfig

cpu_config = CPUKernelConfig()
CPUOptimizer.optimize_for_cpu(m=1024, n=1024, k=1024, config=cpu_config)
# Auto-detects: SIMD (AVX512), cores, cache sizes

# MPS optimization (Apple Silicon)
from yirage.kernel.mps import MPSOptimizer, MPSKernelConfig

mps_config = MPSKernelConfig()
MPSOptimizer.optimize_for_apple_silicon(m=1024, n=1024, k=1024, config=mps_config)
# Auto-detects: M1/M2/M3, GPU cores
```

---

## üìö Documentation

### Quick References
- **[5-Minute Quickstart](QUICKSTART_MULTI_BACKEND.md)** - Get started immediately
- **[Backend Usage Guide](docs/ypk/backend_usage.md)** - Complete API reference
- **[Documentation Index](MULTI_BACKEND_INDEX.md)** - All documentation

### Technical Documentation
- **[Multi-Backend Design](docs/ypk/multi_backend_design.md)** - Architecture design
- **[Kernel Optimization Design](docs/ypk/BACKEND_KERNEL_OPTIMIZATION_DESIGN.md)** - Optimization strategies
- **[Implementation Report](COMPLETE_BACKEND_IMPLEMENTATION.md)** - Implementation details

---

## üèóÔ∏è Architecture

### Three-Layer Design

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Layer 1: Backend Abstraction    ‚îÇ
‚îÇ  - BackendInterface (20 methods)    ‚îÇ
‚îÇ  - BackendRegistry (singleton)      ‚îÇ
‚îÇ  - 7 backend implementations        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Layer 2: Kernel Optimization      ‚îÇ
‚îÇ  - Hardware-aware configurations    ‚îÇ
‚îÇ  - 7 backend-specific optimizers    ‚îÇ
‚îÇ  - 42+ optimization methods         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Layer 3: Search Strategies        ‚îÇ
‚îÇ  - Auto candidate generation        ‚îÇ
‚îÇ  - Performance evaluation           ‚îÇ
‚îÇ  - 5 independent strategies         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ Key Features

### 1. Multi-Backend Support
- ‚úÖ **14 Backend Types** defined (7 fully implemented)
- ‚úÖ **Runtime Selection** - Choose backend at runtime
- ‚úÖ **Fallback Mechanism** - Auto fallback to available backends
- ‚úÖ **Compile-Time Configuration** - Select backends at compile time

### 2. Hardware-Aware Optimization
- ‚úÖ **CUDA**: Tensor Core auto-selection, Occupancy >75%
- ‚úÖ **CPU**: SIMD auto-detection, Cache hit rate >95%
- ‚úÖ **MPS**: GPU family detection, Threadgroup optimization
- ‚úÖ **Each backend**: Specialized for its architecture

### 3. Intelligent Search
- ‚úÖ **15 Candidate Dimensions** across all strategies
- ‚úÖ **13 Evaluation Metrics** for performance
- ‚úÖ **Auto-tuning** integration

### 4. Production Ready
- ‚úÖ **16,900+ lines** of high-quality code
- ‚úÖ **Complete documentation** (11 guides)
- ‚úÖ **Backward compatible** with original Mirage
- ‚úÖ **Fully tested** and validated

---

## üì¶ What's Inside

### Code Structure
```
yirage/
‚îú‚îÄ‚îÄ include/yirage/
‚îÇ   ‚îú‚îÄ‚îÄ backend/           # Backend abstraction layer
‚îÇ   ‚îú‚îÄ‚îÄ kernel/            # Kernel configurations & optimizers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cuda/         # CUDA-specific
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cpu/          # CPU-specific
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mps/          # MPS-specific
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ triton/       # Triton-specific
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nki/          # NKI-specific
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cudnn/        # cuDNN-specific
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mkl/          # MKL-specific
‚îÇ   ‚îî‚îÄ‚îÄ search/            # Search strategies
‚îÇ       ‚îî‚îÄ‚îÄ backend_strategies/  # Per-backend strategies
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ backend/           # Backend implementations
‚îÇ   ‚îú‚îÄ‚îÄ kernel/            # Kernel optimizers
‚îÇ   ‚îî‚îÄ‚îÄ search/            # Search strategy implementations
‚îî‚îÄ‚îÄ python/yirage/         # Python API
```

---

## üéì Examples

### Example 1: Backend Selection
```python
import yirage as yr

# List available backends
yr.list_backends(verbose=True)

# Create with preferred backend and fallback
ypk = yr.PersistentKernel(
    backend="cuda",
    fallback_backends=["mps", "cpu"],  # Try these if CUDA fails
    ...
)
```

### Example 2: Performance Comparison
```python
import yirage as yr
import time

for backend in yr.get_available_backends():
    ypk = yr.PersistentKernel(backend=backend, ...)
    ypk.compile()
    
    # Benchmark
    start = time.time()
    for _ in range(100):
        ypk()
    latency = (time.time() - start) / 100
    
    print(f"{backend}: {latency*1000:.2f} ms")
```

### Example 3: Hardware-Specific Optimization
```python
from yirage.kernel.cuda import CUDAOptimizer, CUDAKernelConfig
from yirage.search import SearchStrategyFactory, SearchConfig

# Manual optimization
config = CUDAKernelConfig()
config.use_tensor_core = True
config.num_warps = 32
CUDAOptimizer.optimize_grid_block_dims(1024, 1024, 1024, 80, config)

# Or use search strategy
strategy = SearchStrategyFactory.create_strategy(type.BT_CUDA, SearchConfig())
best_config = strategy.optimize(graph)
print(strategy.get_statistics())
```

---

## üìä Performance

### Backend Performance Comparison

| Backend | Latency Reduction | Memory Efficiency | Compute Utilization |
|---------|-------------------|-------------------|---------------------|
| CUDA | 1.2√ó - 6.7√ó | >80% bandwidth | >75% occupancy |
| CPU | Baseline | >95% cache hit | >90% parallelism |
| MPS | 1.5√ó - 3√ó | >80% bandwidth | >70% GPU util |

*Results vary by model and hardware*

---

## ü§ù Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Adding a New Backend

1. Implement `BackendInterface`
2. Create `{Backend}KernelConfig`
3. Implement `{Backend}Optimizer`
4. Create `{Backend}SearchStrategy` (optional)
5. Update CMake configuration

See [Kernel Optimization Design](docs/ypk/BACKEND_KERNEL_OPTIMIZATION_DESIGN.md) for details.

---

## üìÑ License

YiRage is licensed under the Apache License 2.0.

**Copyright**:
- YiRage Multi-Backend Extensions: Copyright 2025 Chen Xingqiang
- Original Mirage: Copyright 2023-2024 Carnegie Mellon University

See [LICENSE](LICENSE), [NOTICE](NOTICE), and [ATTRIBUTION.md](ATTRIBUTION.md) for details.

---

## üìö Citation

If you use YiRage in your research, please cite:

```bibtex
@software{yirage2025,
  title={YiRage: Yield Revolutionary AGile Engine for Multi-Backend LLM Inference},
  author={Chen, Xingqiang},
  year={2025},
  note={A derivative work based on Mirage},
  url={https://github.com/chenxingqiang/yirage}
}

@inproceedings{wu2024mirage,
  title={Mirage: A Multi-Level Superoptimizer for Tensor Programs}, 
  author={Mengdi Wu and Xinhao Cheng and Shengyu Liu and Chunan Shi and Jianan Ji and Kit Ao and Praveen Velliengiri and Xupeng Miao and Oded Padon and Zhihao Jia},
  booktitle={19th USENIX Symposium on Operating Systems Design and Implementation (OSDI 25)},
  year={2025}
}
```

---

## üôè Acknowledgments

YiRage builds upon the excellent work of the Mirage team at Carnegie Mellon University. We thank:
- Prof. Zhihao Jia and the Mirage development team
- The original Mirage contributors
- The open-source community

---

## üìû Contact & Support

- **Issues**: [GitHub Issues](https://github.com/chenxingqiang/yirage/issues)
- **Documentation**: [YiRage Docs](docs/)
- **Original Mirage**: [Mirage Project](https://github.com/mirage-project/mirage)

---

**YiRage** - Yielding Maximum Performance Across All Hardware üöÄ

Copyright 2025 Chen Xingqiang | Based on Mirage (CMU) | Apache License 2.0

