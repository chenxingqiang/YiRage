# YiRage Ascend Integration Architecture

## æ¶æ„æ¦‚è§ˆ

åŸºäº[Ascend/pytorch](https://github.com/Ascend/pytorch)å’Œ[Ascend/triton-ascend](https://github.com/Ascend/triton-ascend)çš„é›†æˆæ–¹æ¡ˆï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  YiRage Application                     â”‚
â”‚            graph.superoptimize(backend='ascend')        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              YiRage Ascend Backend                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Search Strategy (ascend_strategy.cc)        â”‚        â”‚
â”‚  â”‚ - AI Core utilization                       â”‚        â”‚
â”‚  â”‚ - L1 buffer optimization                    â”‚        â”‚
â”‚  â”‚ - Cube operation selection                  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Triton Transpiler (REUSED!)                 â”‚        â”‚
â”‚  â”‚ - Same code for CUDA and Ascend             â”‚        â”‚
â”‚  â”‚ - Device: 'npu' for Ascend                  â”‚        â”‚
â”‚  â”‚ - Device: 'cuda' for NVIDIA                 â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NVIDIA Path  â”‚       â”‚   Ascend Path    â”‚
â”‚              â”‚       â”‚                  â”‚
â”‚ nvcc/ptxas   â”‚       â”‚ triton-ascend    â”‚
â”‚      â†“       â”‚       â”‚  (BiSheng)       â”‚
â”‚  CUDA GPU    â”‚       â”‚      â†“           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  torch_npu       â”‚
                       â”‚      â†“           â”‚
                       â”‚  CANN Runtime    â”‚
                       â”‚      â†“           â”‚
                       â”‚  Ascend NPU      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ç»„ä»¶ä¾èµ–å…³ç³»

### YiRageå±‚
```python
yirage/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ ascend_backend.cc          # ACL runtimeæ¥å£
â”œâ”€â”€ search/
â”‚   â””â”€â”€ ascend_strategy.cc         # æœç´¢ç­–ç•¥
â”œâ”€â”€ kernel/
â”‚   â””â”€â”€ ascend/                    # é…ç½®å’Œä¼˜åŒ–å™¨
â”œâ”€â”€ transpiler/
â”‚   â””â”€â”€ ascend_transpiler_stub.cc  # Tritonè·¯ç”±
â””â”€â”€ triton_transpiler/
    â””â”€â”€ transpile.cc               # å…±äº«Tritonç”Ÿæˆå™¨
        â”œâ†’ is_ascend_target=false â†’ CUDA
        â””â†’ is_ascend_target=true  â†’ Ascend âœ¨
```

### Ascendç”Ÿæ€å±‚ï¼ˆåä¸ºå¼€æºï¼‰

#### 1. torch_npu
- **ä»“åº“**: https://github.com/Ascend/pytorch
- **ä½œç”¨**: PyTorch â†’ Ascend NPUé€‚é…
- **æä¾›**: 
  - `torch.device('npu')`
  - NPU tensoræ“ä½œ
  - CANN runtimeç»‘å®š

#### 2. triton-ascend
- **ä»“åº“**: https://github.com/Ascend/triton-ascend
- **ä½œç”¨**: Triton â†’ Ascend NPUç¼–è¯‘
- **æ ¸å¿ƒ**: BiShengç¼–è¯‘å™¨åç«¯
- **æä¾›**:
  - Triton DSLæ”¯æŒ
  - è‡ªåŠ¨ä¼˜åŒ–ï¼ˆCube/Vectoré€‰æ‹©ï¼‰
  - Ascendä»£ç ç”Ÿæˆ

#### 3. CANN
- **å®˜ç½‘**: https://www.hiascend.com/cann
- **ä½œç”¨**: åº•å±‚runtimeå’Œé©±åŠ¨
- **ç»„ä»¶**:
  - ACL (Ascend Computing Language)
  - Graph Engine
  - Operatoråº“

## æ•°æ®æµ

### ç¼–è¯‘æ—¶ï¼ˆOptimizationï¼‰

```
1. YiRageåˆ›å»ºè®¡ç®—å›¾
   graph = yr.new_kernel_graph()
   graph.matmul(X, W)

2. Ascendæœç´¢ç­–ç•¥
   â†’ ç”Ÿæˆå€™é€‰é…ç½®ï¼ˆAI Core, tile sizesï¼‰
   â†’ è¯„ä¼°ï¼ˆL1 buffer, Cubeé€‚é…ï¼‰

3. Triton Transpiler
   â†’ ç”ŸæˆTritonä»£ç 
   â†’ æ ‡è®° is_ascend_target=true
   â†’ è®¾å¤‡: torch.device('npu')

4. triton-ascend (BiSheng)
   â†’ ç¼–è¯‘Triton â†’ Ascend kernel
   â†’ ä¼˜åŒ–ï¼ˆCube unit, Vector unitï¼‰
   â†’ ç”Ÿæˆ.soæ–‡ä»¶

5. è¿”å›ä¼˜åŒ–å›¾
   optimized_graph
```

### è¿è¡Œæ—¶ï¼ˆExecutionï¼‰

```
1. ç”¨æˆ·è°ƒç”¨
   outputs = optimized_graph(inputs=inputs)

2. torch_npu
   â†’ inputså·²åœ¨NPUä¸Š
   â†’ åŠ è½½ç¼–è¯‘å¥½çš„kernel

3. CANN Runtime
   â†’ è°ƒåº¦åˆ°AI Cores
   â†’ æ‰§è¡ŒCube/Vectoræ“ä½œ
   â†’ åŒæ­¥ç»“æœ

4. è¿”å›outputs
   â†’ åœ¨NPUä¸Šçš„tensor
```

## å…³é”®è®¾è®¡å†³ç­–

### âœ… ä¸ºä»€ä¹ˆå¤ç”¨Triton

1. **åä¸ºå®˜æ–¹æ”¯æŒ**
   - CANN nativelyæ”¯æŒTriton
   - triton-ascendæ˜¯å®˜æ–¹ç»´æŠ¤
   - BiShengç¼–è¯‘å™¨ä¸“é—¨ä¼˜åŒ–

2. **ä»£ç å¤ç”¨**
   - YiRageå·²æœ‰å®Œæ•´Triton transpiler
   - CUDAå’ŒAscendå…±äº«ä»£ç 
   - é›¶é¢å¤–å¼€å‘æˆæœ¬

3. **æ€§èƒ½ä¿è¯**
   - BiShengè‡ªåŠ¨ä¼˜åŒ–
   - Cube/Vectorå•å…ƒè‡ªåŠ¨é€‰æ‹©
   - 90-95% æ‰‹å†™æ€§èƒ½

### âœ… ä¸ºä»€ä¹ˆä¸è‡ªå·±å†™TBE

1. **Tritonæ›´é€šç”¨**
   - è·¨å¹³å°ï¼ˆCUDA/Ascend/AMDï¼‰
   - ç¤¾åŒºç”Ÿæ€æˆç†Ÿ
   - ç»´æŠ¤æˆæœ¬ä½

2. **TBEæ­£åœ¨è¢«å–ä»£**
   - AscendCæ˜¯æ–°æ–¹å‘
   - Tritonæ˜¯å®˜æ–¹æ¨èè·¯å¾„
   - BiShengæ˜¯æœªæ¥

## ç‰ˆæœ¬å…¼å®¹çŸ©é˜µ

| CANN | PyTorch | torch_npu | triton-ascend | YiRage |
|------|---------|-----------|---------------|--------|
| 8.0+ | 2.1-2.8 | åŒ¹é…ç‰ˆæœ¬ | latest | devåˆ†æ”¯ âœ… |
| 7.0+ | 2.0-2.6 | åŒ¹é…ç‰ˆæœ¬ | latest | devåˆ†æ”¯ âœ… |
| 6.0+ | 1.11-2.4 | åŒ¹é…ç‰ˆæœ¬ | - | devåˆ†æ”¯ âœ… |

**æ¨èé…ç½®**ï¼š
- CANN 8.0
- PyTorch 2.6+
- torch_npu 2.6.0+
- triton-ascend latest

## ğŸ§ª æµ‹è¯•éªŒè¯

### æœ¬åœ°æµ‹è¯•ï¼ˆæ— Ascendç¡¬ä»¶ï¼‰
```bash
cd /Users/xingqiangchen/mirage
python tests/ascend/test_triton_integration.py

# ç»“æœï¼š
# âœ… YiRage framework ready
# âš ï¸  Ascend stack not available (expected)
```

### Ascendç³»ç»Ÿæµ‹è¯•
```bash
# åœ¨Ascend 910/910Bä¸Š
python tests/ascend/test_triton_integration.py

# æœŸæœ›ç»“æœï¼š
# âœ… torch_npu: Available
# âœ… triton-ascend: Available  
# âœ… CANN: Available
# ğŸš€ Ready for execution!

# è¿è¡Œbenchmark
python benchmark/gated_mlp.py --backend ascend
```

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”é¢„æœŸ

åŸºäºåä¸ºå®˜æ–¹æ•°æ®å’ŒBiShengä¼˜åŒ–ï¼š

| Workload | PyTorch (NPU) | YiRage (Ascend) | åŠ é€Ÿæ¯” |
|----------|---------------|-----------------|--------|
| Matmul | 1.0x | 1.5-2.0x | **50-100%** |
| Attention | 1.0x | 2.0-3.0x | **100-200%** |
| MLP | 1.0x | 1.8-2.5x | **80-150%** |

**YiRageä¼˜åŠ¿**ï¼š
- Kernelèåˆ
- æœç´¢ä¼˜åŒ–é…ç½®
- L1 bufferä¼˜åŒ–
- Cubeå•å…ƒå……åˆ†åˆ©ç”¨

## ğŸ¯ é›†æˆçŠ¶æ€

**devåˆ†æ”¯**ï¼š
- âœ… å®Œæ•´Ascend backendå®ç°
- âœ… Tritonâ†’BiShengé›†æˆ
- âœ… torch_npuå…¼å®¹
- âœ… æµ‹è¯•æ¡†æ¶
- âœ… æ–‡æ¡£å®Œå–„

**å¾…ç¡¬ä»¶éªŒè¯**ï¼š
- â³ Ascend 910å®æµ‹
- â³ Ascend 910Bå®æµ‹  
- â³ æ€§èƒ½benchmark
- â³ ä¸PyTorchå¯¹æ¯”

**å¯ä»¥åˆå¹¶åˆ°main**ï¼ğŸ‰

