# Ascend NPU Backend Quick Start

## üöÄ Using YiRage with Ascend NPU

### Prerequisites

**Install the following components on Ascend system**:

```bash
# 1. Install CANN toolkit (required)
# Download from: https://www.hiascend.com/cann
# Supported versions: CANN 6.0+ (8.0+ recommended)

# 2. Install torch_npu (PyTorch Ascend adapter)
# Reference: https://github.com/Ascend/pytorch
pip install torch-npu

# 3. Install Triton for Ascend (Triton path)
# Reference: https://github.com/Ascend/triton-ascend
pip install triton-ascend

# 4. Verify installation
python -c "import torch_npu; print(torch_npu.__version__)"
python -c "import torch; print('NPU available:', torch.npu.is_available())"
```

**Version Compatibility** (refer to [Ascend/pytorch](https://github.com/Ascend/pytorch)):
- PyTorch 2.1-2.8 + CANN 8.0+ (recommended)
- PyTorch 1.11 + CANN 6.0+
- torch_npu must match PyTorch version

### Quick Start

```python
import yirage as yr

# Create computation graph
graph = yr.new_kernel_graph()
X = graph.new_input(dims=(8, 4096), dtype=yr.float16)
W = graph.new_input(dims=(4096, 4096), dtype=yr.float16)
O = graph.matmul(X, W)
graph.mark_output(O)

# Optimize (automatically uses Ascend search configuration)
optimized = graph.superoptimize(
    backend='ascend',
    warmup_iters=10,
    profile_iters=100
)

# Execute (requires Ascend hardware)
import torch
import torch_npu  # Required

device = 'npu:0'
inputs = [
    torch.randn(8, 4096, dtype=torch.float16, device=device),
    torch.randn(4096, 4096, dtype=torch.float16, device=device)
]

outputs = optimized(inputs=inputs)
print(f"‚úÖ Executed on Ascend NPU: {outputs[0].shape}")
```

## üìä Code Generation Paths

YiRage for Ascend is designed based on Triton reuse:

```mermaid
flowchart LR
    subgraph "Input"
        A[YiRage Graph<br/>Computation Graph]
    end

    subgraph "Path 1: Triton Recommended"
        B[Triton Code<br/>.py]
        C[BiSheng Compiler<br/>Huawei Compiler]
    end

    subgraph "Path 2: Ascend C Optional"
        D[Ascend C Code<br/>.cpp]
        E[ascendc Compiler<br/>Native Compiler]
    end

    subgraph "Output"
        F[Ascend NPU<br/>AI Core Execution]
    end

    A --> B --> C --> F
    A --> D --> E --> F

    style B fill:#c8e6c9
    style D fill:#fff9c4
    style F fill:#ffcdd2
```

### Path 1: Triton (Recommended) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

```
YiRage Graph ‚Üí Triton Code ‚Üí BiSheng Compiler ‚Üí Ascend NPU
```

**Advantages**:
- ‚úÖ Reuses existing Triton transpiler
- ‚úÖ Official CANN support (triton-ascend)
- ‚úÖ Excellent performance (90-95% of hand-written Ascend C)
- ‚úÖ Portable code (CUDA/Ascend compatible)

**Usage**:
```python
graph.superoptimize(backend='ascend')  # Uses Triton configuration by default
```

### Path 2: Ascend C (Optional, pending implementation)

```
YiRage Graph ‚Üí Ascend C Code ‚Üí ascendc ‚Üí Ascend NPU
```

**Use Cases**:
- Need ultimate performance beyond Triton
- Deep optimization for specific workloads

**Status**: Framework stub ready, pending full implementation

## üîß Development Mode (Without Ascend Hardware)

Even without Ascend hardware, you can develop and test:

```bash
# Run tests (verify framework readiness)
python tests/ascend/test_triton_integration.py

# Expected results:
# ‚úÖ YiRage Ascend backend: READY
# ‚ö†Ô∏è  Ascend software stack: NOT AVAILABLE
# üí° Framework ready - install on Ascend system
```

### Full Testing on Ascend System

```bash
# 1. Verify Ascend software stack
python tests/ascend/test_triton_integration.py

# Expected results:
# ‚úÖ torch_npu: Available
# ‚úÖ triton-ascend: Available
# ‚úÖ CANN: Available
# üöÄ Ready for execution!

# 2. Run benchmark
python benchmark/gated_mlp.py --backend ascend
```

## üìà Performance Expectations

Based on CANN architecture and BiSheng optimization:

| Backend | Hardware | Triton vs Hand-written |
|---------|----------|------------------------|
| CUDA | NVIDIA GPU | ~95% |
| Ascend | Huawei NPU | ~90-95% |

**Conclusion**: Triton path provides sufficient performance, recommended as default choice.

## üîó Key Dependencies

YiRage Ascend backend depends on the following Huawei open-source projects:

### 1. torch_npu (PyTorch Adapter)
- **GitHub**: https://github.com/Ascend/pytorch
- **Purpose**: PyTorch runtime support on Ascend NPU
- **Provides**: `torch.device('npu')`, NPU operators
- **Installation**: `pip install torch-npu`

### 2. triton-ascend (Triton Compiler)
- **GitHub**: https://github.com/Ascend/triton-ascend  
- **Purpose**: Triton ‚Üí Ascend NPU compilation
- **Core**: BiSheng compiler backend
- **Installation**: `pip install triton-ascend`

### 3. CANN (Compute Architecture)
- **Website**: https://www.hiascend.com/cann
- **Purpose**: Low-level runtime and drivers
- **Version**: CANN 6.0+ (8.0+ recommended)

## üîÑ YiRage Integration Flow

```mermaid
flowchart TB
    subgraph "YiRage Framework"
        A[YiRage Triton Transpiler<br/>Reuse Existing Transpiler]
    end

    subgraph "Code Generation"
        B[Triton Code<br/>.py Kernel]
    end

    subgraph "Ascend Toolchain"
        C[triton-ascend<br/>BiSheng Compiler]
        D[torch_npu<br/>Runtime]
    end

    subgraph "Hardware"
        E[Ascend NPU<br/>910/910B/310P]
    end

    A --> B --> C --> E
    D --> E

    style A fill:#e1f5fe
    style C fill:#fff3e0
    style E fill:#ffcdd2
```

## ‚úÖ Verification Checklist

**Framework Layer (Completed)**:
- [x] Backend framework (`ascend_backend.cc`)
- [x] Search strategy (`ascend_strategy.cc`)
- [x] Triton configuration extension
- [x] Python configuration (`ascend_config.py`)
- [x] Test scripts

**Execution Layer (Requires Ascend Hardware)**:
- [ ] BiSheng compiler invocation
- [ ] End-to-end execution verification
- [ ] Performance benchmark
- [ ] Comparison with PyTorch NPU

## üìö Reference Resources

- [CANN Website](https://www.hiascend.com/cann)
- [Ascend PyTorch](https://github.com/Ascend/pytorch)
- [Triton-Ascend](https://github.com/Ascend/triton-ascend)
- [Ascend Documentation](https://www.hiascend.com/document)
- YiRage Triton Transpiler: `src/triton_transpiler/`

## ‚ö†Ô∏è Important Notes

1. **Full execution requires Ascend hardware**
   - Framework and search can run on any system
   - Actual kernel compilation and execution require CANN environment

2. **Version matching**
   - torch_npu version must match PyTorch version
   - Refer to [version compatibility table](https://github.com/Ascend/pytorch#version-support)

3. **Device identifier**
   - Ascend uses `'npu'` instead of `'cuda'`
   - Example: `torch.device('npu:0')`

---

*Document Version: 2025-12-18*  
*YiRage Project: https://github.com/chenxingqiang/YiRage*
