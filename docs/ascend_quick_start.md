# Ascend NPU Backend Quick Start

## ğŸš€ ä½¿ç”¨YiRage + Ascend NPU

### å‰ææ¡ä»¶

**åœ¨Ascendç³»ç»Ÿä¸Šå®‰è£…ä»¥ä¸‹ç»„ä»¶**ï¼š

```bash
# 1. å®‰è£…CANNå·¥å…·åŒ…ï¼ˆå¿…éœ€ï¼‰
# ä¸‹è½½è‡ª: https://www.hiascend.com/cann
# æ”¯æŒç‰ˆæœ¬: CANN 6.0+ (æ¨è 8.0+)

# 2. å®‰è£…torch_npuï¼ˆPyTorch Ascendé€‚é…å™¨ï¼‰
# å‚è€ƒ: https://github.com/Ascend/pytorch
pip install torch-npu

# 3. å®‰è£…Triton for Ascendï¼ˆTritonè·¯å¾„ï¼‰
# å‚è€ƒ: https://github.com/Ascend/triton-ascend
pip install triton-ascend

# 4. éªŒè¯å®‰è£…
python -c "import torch_npu; print(torch_npu.__version__)"
python -c "import torch; print('NPU available:', torch.npu.is_available())"
```

**ç‰ˆæœ¬å…¼å®¹æ€§**ï¼ˆå‚è€ƒ[Ascend/pytorch](https://github.com/Ascend/pytorch)ï¼‰ï¼š
- PyTorch 2.1-2.8 + CANN 8.0+ (æ¨è)
- PyTorch 1.11 + CANN 6.0+
- torch_npuéœ€åŒ¹é…PyTorchç‰ˆæœ¬

### å¿«é€Ÿå¼€å§‹

```python
import yirage as yr

# åˆ›å»ºè®¡ç®—å›¾
graph = yr.new_kernel_graph()
X = graph.new_input(dims=(8, 4096), dtype=yr.float16)
W = graph.new_input(dims=(4096, 4096), dtype=yr.float16)
O = graph.matmul(X, W)
graph.mark_output(O)

# ä¼˜åŒ–ï¼ˆè‡ªåŠ¨ä½¿ç”¨Ascendæœç´¢é…ç½®ï¼‰
optimized = graph.superoptimize(
    backend='ascend',
    warmup_iters=10,
    profile_iters=100
)

# æ‰§è¡Œï¼ˆéœ€è¦Ascendç¡¬ä»¶ï¼‰
import torch
import torch_npu  # å¿…éœ€

device = 'npu:0'
inputs = [
    torch.randn(8, 4096, dtype=torch.float16, device=device),
    torch.randn(4096, 4096, dtype=torch.float16, device=device)
]

outputs = optimized(inputs=inputs)
print(f"âœ… Executed on Ascend NPU: {outputs[0].shape}")
```

## ğŸ“Š ä»£ç ç”Ÿæˆè·¯å¾„

YiRage for Ascendçš„è®¾è®¡åŸºäºTritonå¤ç”¨ï¼š

```mermaid
flowchart LR
    subgraph "Input"
        A[YiRage Graph<br/>è®¡ç®—å›¾]
    end

    subgraph "Path 1: Triton æ¨è"
        B[Triton Code<br/>.py]
        C[BiSheng Compiler<br/>åä¸ºç¼–è¯‘å™¨]
    end

    subgraph "Path 2: Ascend C å¯é€‰"
        D[Ascend C Code<br/>.cpp]
        E[ascendc Compiler<br/>åŸç”Ÿç¼–è¯‘å™¨]
    end

    subgraph "Output"
        F[Ascend NPU<br/>AI Coreæ‰§è¡Œ]
    end

    A --> B --> C --> F
    A --> D --> E --> F

    style B fill:#c8e6c9
    style D fill:#fff9c4
    style F fill:#ffcdd2
```

### Path 1: Tritonï¼ˆæ¨èï¼‰â­â­â­â­â­

```
YiRage Graph â†’ Triton Code â†’ BiSheng Compiler â†’ Ascend NPU
```

**ä¼˜åŠ¿**ï¼š
- âœ… å¤ç”¨ç°æœ‰Triton transpiler
- âœ… CANNå®˜æ–¹æ”¯æŒï¼ˆtriton-ascendï¼‰
- âœ… æ€§èƒ½ä¼˜ç§€ï¼ˆ90-95% æ‰‹å†™Ascend Cï¼‰
- âœ… ä»£ç å¯ç§»æ¤ï¼ˆCUDA/Ascendé€šç”¨ï¼‰

**ä½¿ç”¨**ï¼š
```python
graph.superoptimize(backend='ascend')  # é»˜è®¤ä½¿ç”¨Tritoné…ç½®
```

### Path 2: Ascend Cï¼ˆå¯é€‰ï¼Œå¾…å®ç°ï¼‰

```
YiRage Graph â†’ Ascend C Code â†’ ascendc â†’ Ascend NPU
```

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦è¶…è¶ŠTritonçš„æè‡´æ€§èƒ½
- é’ˆå¯¹ç‰¹å®šworkloadæ·±åº¦ä¼˜åŒ–

**çŠ¶æ€**ï¼šæ¡†æ¶stubå°±ç»ªï¼Œå¾…å®Œæ•´å®ç°

## ğŸ”§ å¼€å‘æ¨¡å¼ï¼ˆæ— Ascendç¡¬ä»¶ï¼‰

å³ä½¿æ²¡æœ‰Ascendç¡¬ä»¶ï¼Œä¹Ÿå¯ä»¥è¿›è¡Œå¼€å‘å’Œæµ‹è¯•ï¼š

```bash
# è¿è¡Œæµ‹è¯•ï¼ˆéªŒè¯æ¡†æ¶å°±ç»ªï¼‰
python tests/ascend/test_triton_integration.py

# é¢„æœŸç»“æœï¼š
# âœ… YiRage Ascend backend: READY
# âš ï¸  Ascend software stack: NOT AVAILABLE
# ğŸ’¡ Framework ready - install on Ascend system
```

### åœ¨Ascendç³»ç»Ÿä¸Šå®Œæ•´æµ‹è¯•

```bash
# 1. éªŒè¯Ascendè½¯ä»¶æ ˆ
python tests/ascend/test_triton_integration.py

# æœŸæœ›ç»“æœï¼š
# âœ… torch_npu: Available
# âœ… triton-ascend: Available
# âœ… CANN: Available
# ğŸš€ Ready for execution!

# 2. è¿è¡Œbenchmark
python benchmark/gated_mlp.py --backend ascend
```

## ğŸ“ˆ æ€§èƒ½é¢„æœŸ

åŸºäºCANNæ¶æ„å’ŒBiShengä¼˜åŒ–ï¼š

| Backend | ç¡¬ä»¶ | Triton vs æ‰‹å†™ |
|---------|------|---------------|
| CUDA | NVIDIA GPU | ~95% |
| Ascend | åä¸ºNPU | ~90-95% |

**ç»“è®º**ï¼šTritonè·¯å¾„æ€§èƒ½å……è¶³ï¼Œæ¨èä½œä¸ºé»˜è®¤é€‰æ‹©ã€‚

## ğŸ”— å…³é”®ä¾èµ–

YiRage Ascend backendä¾èµ–ä»¥ä¸‹åä¸ºå¼€æºé¡¹ç›®ï¼š

### 1. torch_npu (PyTorché€‚é…å™¨)
- **GitHub**: https://github.com/Ascend/pytorch
- **ç”¨é€”**: PyTorchåœ¨Ascend NPUä¸Šçš„è¿è¡Œæ—¶æ”¯æŒ
- **æä¾›**: `torch.device('npu')`, NPUç®—å­
- **å®‰è£…**: `pip install torch-npu`

### 2. triton-ascend (Tritonç¼–è¯‘å™¨)
- **GitHub**: https://github.com/Ascend/triton-ascend  
- **ç”¨é€”**: Triton â†’ Ascend NPUç¼–è¯‘
- **æ ¸å¿ƒ**: BiShengç¼–è¯‘å™¨åç«¯
- **å®‰è£…**: `pip install triton-ascend`

### 3. CANN (è®¡ç®—æ¶æ„)
- **å®˜ç½‘**: https://www.hiascend.com/cann
- **ç”¨é€”**: åº•å±‚runtimeå’Œé©±åŠ¨
- **ç‰ˆæœ¬**: CANN 6.0+ (æ¨è 8.0+)

## ğŸ”„ YiRageé›†æˆæ–¹å¼

```mermaid
flowchart TB
    subgraph "YiRage Framework"
        A[YiRage Triton Transpiler<br/>å¤ç”¨ç°æœ‰è½¬è¯‘å™¨]
    end

    subgraph "Code Generation"
        B[Triton Code<br/>.py Kernel]
    end

    subgraph "Ascend Toolchain"
        C[triton-ascend<br/>BiSheng Compiler]
        D[torch_npu<br/>Runtime]
    end

    subgraph "Hardware"
        E[Ascend NPU<br/>910/910B/310P]
    end

    A --> B --> C --> E
    D --> E

    style A fill:#e1f5fe
    style C fill:#fff3e0
    style E fill:#ffcdd2
```

## âœ… éªŒè¯æ¸…å•

**æ¡†æ¶å±‚ï¼ˆå·²å®Œæˆï¼‰**ï¼š
- [x] Backendæ¡†æ¶ (`ascend_backend.cc`)
- [x] æœç´¢ç­–ç•¥ (`ascend_strategy.cc`)
- [x] Tritoné…ç½®æ‰©å±•
- [x] Pythoné…ç½® (`ascend_config.py`)
- [x] æµ‹è¯•è„šæœ¬

**æ‰§è¡Œå±‚ï¼ˆéœ€Ascendç¡¬ä»¶ï¼‰**ï¼š
- [ ] BiShengç¼–è¯‘å™¨è°ƒç”¨
- [ ] ç«¯åˆ°ç«¯æ‰§è¡ŒéªŒè¯
- [ ] æ€§èƒ½benchmark
- [ ] ä¸PyTorch NPUå¯¹æ¯”

## ğŸ“š å‚è€ƒèµ„æº

- [CANNå®˜ç½‘](https://www.hiascend.com/cann)
- [Ascend PyTorch](https://github.com/Ascend/pytorch)
- [Triton-Ascend](https://github.com/Ascend/triton-ascend)
- [Ascendæ–‡æ¡£](https://www.hiascend.com/document)
- YiRage Triton Transpiler: `src/triton_transpiler/`

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å®Œæ•´æ‰§è¡Œéœ€è¦Ascendç¡¬ä»¶**
   - æ¡†æ¶å’Œæœç´¢å¯åœ¨ä»»æ„ç³»ç»Ÿè¿è¡Œ
   - å®é™…kernelç¼–è¯‘å’Œæ‰§è¡Œéœ€è¦CANNç¯å¢ƒ

2. **ç‰ˆæœ¬åŒ¹é…**
   - torch_npuç‰ˆæœ¬å¿…é¡»ä¸PyTorchç‰ˆæœ¬åŒ¹é…
   - å‚è€ƒ[ç‰ˆæœ¬å…¼å®¹è¡¨](https://github.com/Ascend/pytorch#version-support)

3. **è®¾å¤‡æ ‡è¯†**
   - Ascendä½¿ç”¨ `'npu'` è€Œé `'cuda'`
   - ä¾‹å¦‚: `torch.device('npu:0')`
