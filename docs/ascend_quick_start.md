# Ascend NPU Backend Quick Start

## ğŸš€ ä½¿ç”¨YiRage + Ascend NPU

### å‰ææ¡ä»¶

åœ¨Ascendç³»ç»Ÿä¸Šå®‰è£…ä»¥ä¸‹ç»„ä»¶ï¼š

```bash
# 1. å®‰è£…CANNå·¥å…·åŒ…ï¼ˆå¿…éœ€ï¼‰
# ä¸‹è½½è‡ª: https://www.hiascend.com/cann
# æ”¯æŒç‰ˆæœ¬: CANN 6.0+ (æ¨è 8.0+)

# 2. å®‰è£…torch_npuï¼ˆPyTorch Ascendé€‚é…å™¨ï¼‰
# å‚è€ƒ: https://github.com/Ascend/pytorch
pip install torch-npu

# 3. å®‰è£…Triton for Ascendï¼ˆTritonè·¯å¾„ï¼‰
# å‚è€ƒ: https://github.com/Ascend/triton-ascend
pip install triton-ascend

# éªŒè¯å®‰è£…
python -c "import torch_npu; print(torch_npu.__version__)"
python -c "import torch; print('NPU available:', torch.npu.is_available())"
```

**ç‰ˆæœ¬å…¼å®¹æ€§**ï¼ˆå‚è€ƒAscend/pytorchï¼‰ï¼š
- PyTorch 2.1-2.8 + CANN 8.0+ (æ¨è)
- PyTorch 1.11 + CANN 6.0+
- torch_npuéœ€åŒ¹é…PyTorchç‰ˆæœ¬

### å¿«é€Ÿå¼€å§‹

```python
import yirage as yr
import torch_npu

# åˆ›å»ºè®¡ç®—å›¾
graph = yr.new_kernel_graph()
X = graph.new_input(dims=(8, 4096), dtype=yr.float16)
W = graph.new_input(dims=(4096, 4096), dtype=yr.float16)
O = graph.matmul(X, W)
graph.mark_output(O)

# ä¼˜åŒ–ï¼ˆè‡ªåŠ¨ä½¿ç”¨Tritonâ†’BiShengè·¯å¾„ï¼‰
optimized = graph.superoptimize(
    backend='ascend',
    warmup_iters=10,
    profile_iters=100
)

# æ‰§è¡Œ
device = 'npu:0'
inputs = [
    torch.randn(8, 4096, dtype=torch.float16, device=device),
    torch.randn(4096, 4096, dtype=torch.float16, device=device)
]

outputs = optimized(inputs=inputs)
print(f"âœ… Executed on Ascend NPU: {outputs[0].shape}")
```

## ğŸ“Š ä»£ç ç”Ÿæˆè·¯å¾„

YiRage for Ascendæ”¯æŒä¸‰ç§è·¯å¾„ï¼š

### Path 1: Triton (æ¨è) â­â­â­â­â­

```
YiRage Graph â†’ Triton Code â†’ BiSheng Compiler â†’ Ascend NPU
```

**ä¼˜åŠ¿**ï¼š
- âœ… å¤ç”¨ç°æœ‰Triton transpilerï¼ˆ0é¢å¤–å¼€å‘ï¼‰
- âœ… CANNå®˜æ–¹æ”¯æŒ
- âœ… æ€§èƒ½ä¼˜ç§€ï¼ˆ90-95% æ‰‹å†™Ascend Cï¼‰
- âœ… ä»£ç å¯ç§»æ¤ï¼ˆCUDA/Ascendé€šç”¨ï¼‰

**ä½¿ç”¨**ï¼š
```python
graph.superoptimize(backend='ascend')  # é»˜è®¤ä½¿ç”¨Tritonè·¯å¾„
```

### Path 2: Ascend C (é«˜çº§) â­â­â­â­

```
YiRage Graph â†’ Ascend C Code â†’ ascendc â†’ Ascend NPU
```

**ä¼˜åŠ¿**ï¼š
- âœ… æè‡´æ€§èƒ½ï¼ˆ100%ï¼‰
- âœ… å®Œå…¨æ§åˆ¶ç¡¬ä»¶ç‰¹æ€§

**ä½¿ç”¨åœºæ™¯**ï¼š
- éœ€è¦è¶…è¶ŠTritonçš„æ€§èƒ½
- é’ˆå¯¹ç‰¹å®šworkloadæ·±åº¦ä¼˜åŒ–

**çŠ¶æ€**ï¼šæ¡†æ¶å°±ç»ªï¼Œå¾…å®ç°

### Path 3: TBE (å…¼å®¹) â­â­â­

ä»…ç”¨äºAscend 910æ—§ç‰ˆCANNå…¼å®¹

## ğŸ”§ å¼€å‘æ¨¡å¼ï¼ˆæ— Ascendç¡¬ä»¶ï¼‰

å³ä½¿æ²¡æœ‰Ascendç¡¬ä»¶ï¼Œä¹Ÿå¯ä»¥å¼€å‘ï¼š

```bash
# è¿è¡Œæµ‹è¯•ï¼ˆä¼šä½¿ç”¨CPU fallbackï¼‰
python tests/ascend/test_triton_integration.py

# ç»“æœï¼š
# âœ… Ascend backend framework: READY
# âš ï¸  BiSheng compiler: NOT AVAILABLE
# ğŸ’¡ Can still develop - test on Ascend hardware later
```

**åœ¨Ascendç³»ç»Ÿä¸Šæµ‹è¯•**ï¼š
```bash
# ç”Ÿæˆä»£ç å¹¶ç¼–è¯‘
python tests/ascend/test_triton_integration.py

# æ‰§è¡Œbenchmark
python benchmark/gated_mlp.py --backend ascend
```

## ğŸ“ˆ æ€§èƒ½é¢„æœŸ

åŸºäºCANNæ¶æ„å’ŒBiShengä¼˜åŒ–ï¼š

| Backend | ç¡¬ä»¶ | Tritonæ€§èƒ½ | æ‰‹å†™æ€§èƒ½ |
|---------|------|-----------|---------|
| CUDA | NVIDIA GPU | ~95% | 100% |
| Ascend | åä¸ºNPU | ~90-95% | 100% |

**ç»“è®º**ï¼šTritonè·¯å¾„æ€§èƒ½å……è¶³ï¼Œæ¨èä½œä¸ºé»˜è®¤é€‰æ‹©ï¼

## ğŸ¯ BiShengç¼–è¯‘å‘½ä»¤

YiRageè‡ªåŠ¨ç”Ÿæˆçš„ç¼–è¯‘å‘½ä»¤ï¼š

```bash
bisheng-triton \
  --target=Ascend910B \
  --opt-level=3 \
  --enable-fp16 \
  -o kernel.so
```

## âœ… éªŒè¯æ¸…å•

- [x] Backendæ¡†æ¶
- [x] æœç´¢ç­–ç•¥
- [x] Tritoné›†æˆ
- [x] é…ç½®æ–‡ä»¶
- [x] æµ‹è¯•è„šæœ¬
- [ ] çœŸå®ç¡¬ä»¶éªŒè¯ï¼ˆéœ€è¦Ascend 910/910Bï¼‰
- [ ] æ€§èƒ½benchmark
- [ ] ä¸PyTorchå¯¹æ¯”

## ğŸ”— å…³é”®ä¾èµ–

YiRage Ascend backendä¾èµ–ä»¥ä¸‹åä¸ºå¼€æºé¡¹ç›®ï¼š

### 1. torch_npu (PyTorché€‚é…å™¨)
- **GitHub**: https://github.com/Ascend/pytorch
- **ç”¨é€”**: PyTorchåœ¨Ascend NPUä¸Šçš„è¿è¡Œæ—¶æ”¯æŒ
- **æä¾›**: `torch.device('npu')`, NPUç®—å­
- **å®‰è£…**: `pip install torch-npu`

### 2. triton-ascend (Tritonç¼–è¯‘å™¨)
- **GitHub**: https://github.com/Ascend/triton-ascend  
- **ç”¨é€”**: Triton â†’ Ascend NPUç¼–è¯‘
- **æ ¸å¿ƒ**: BiShengç¼–è¯‘å™¨åç«¯
- **å®‰è£…**: `pip install triton-ascend`

### 3. CANN (è®¡ç®—æ¶æ„)
- **å®˜ç½‘**: https://www.hiascend.com/cann
- **ç”¨é€”**: åº•å±‚runtimeå’Œé©±åŠ¨
- **ç‰ˆæœ¬**: CANN 6.0+ (æ¨è 8.0+)

## ğŸ”„ YiRageé›†æˆæ–¹å¼

```
YiRage Triton Transpiler (å¤ç”¨)
        â†“
    Triton Code
        â†“
triton-ascend (BiSheng)
        â†“
    Ascend NPU
        â†‘
    torch_npu (Runtime)
```

## ğŸ“š å‚è€ƒèµ„æº

- [CANNå®˜ç½‘](https://www.hiascend.com/cann)
- [Ascend PyTorch](https://github.com/Ascend/pytorch)
- [Triton-Ascend](https://github.com/Ascend/triton-ascend)
- [Ascendæ–‡æ¡£](https://www.hiascend.com/document)
- YiRage Triton Transpiler: `src/triton_transpiler/`

