/* Copyright 2025 Chen Xingqiang (YiRage Project)
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * This file is part of YiRage (Yi Revolutionary AGile Engine)
 * 
 * Ascend Matrix Multiplication Kernel
 * 
 * Matrix multiplication implementation for Huawei Ascend NPU backend.
 * Optimized for Ascend's Cube Unit (16x16 native tiles).
 */

#include "yirage/kernel/device_memory_manager.h"
#include "yirage/kernel/graph.h"
#include "yirage/kernel/matmul.h"
#include "yirage/utils/ascend_helper.h"
#include "yirage/utils/fingerprint_functions.h"
#include "yirage/utils/hash_utils.h"
#include <cassert>

#ifdef __ASCEND__
#include "acl/acl.h"
#endif

namespace yirage {
namespace kernel {

using namespace yirage::type;
using namespace yirage::config;
using namespace yirage::utils;

#ifdef YIRAGE_FINGERPRINT_USE_ASCEND

/**
 * @brief Compute matrix multiplication fingerprint on Ascend
 * 
 * This kernel computes C = A * B fingerprint for verification.
 * Note: Tile sizes should be multiples of 16 for Cube Unit efficiency.
 */
#ifdef __ASCEND__
__global__ 
#endif
void compute_matmul_fingerprint_ascend(yirage::type::FPType *A_ptr,
                                       yirage::type::FPType *B_ptr,
                                       yirage::type::FPType *C_ptr,
                                       int num_batches,
                                       int m,
                                       int n,
                                       int k) {
#ifdef __ASCEND__
  int row_idx = (threadIdx.x + blockIdx.x * blockDim.x) / n;
  int col_idx = (threadIdx.x + blockIdx.x * blockDim.x) % n;
#else
  // CPU fallback for compilation without Ascend
  int row_idx = 0;
  int col_idx = 0;
#endif
  
  int mk = m * k;
  int mn = m * n;
  int nk = n * k;
  
  if (row_idx < m) {
    for (int b = 0; b < num_batches; b++) {
      yirage::type::FPType result = 0;
      for (int i = 0; i < k; i++) {
        yirage::type::FPType x = A_ptr[b * mk + row_idx * k + i];
        yirage::type::FPType y = B_ptr[b * nk + i * n + col_idx];
        yirage::type::FPType z = utils::compute_mul_fingerprint(x, y);
        result = utils::compute_add_fingerprint(result, z);
      }
#ifdef __ASCEND__
      C_ptr[b * mn + threadIdx.x + blockIdx.x * blockDim.x] = result;
#else
      C_ptr[b * mn + row_idx * n + col_idx] = result;
#endif
    }
  }
}

bool KNMatmulOp::fingerprint(void) {
  // Currently assert a single GPU/NPU
  assert(kgraph->gpu_dim.y == 1);
  assert(kgraph->gpu_dim.z == 1);

  int num_dims = input_tensors[0].num_dims;
  int row_A = input_tensors[0].dim[num_dims - 2];
  int column_A = input_tensors[0].dim[num_dims - 1];
  int row_B = input_tensors[1].dim[num_dims - 2];
  int column_B = input_tensors[1].dim[num_dims - 1];
  int row_C = output_tensors[0].dim[num_dims - 2];
  int column_C = output_tensors[0].dim[num_dims - 1];
  assert(column_A == row_B);
  assert(row_C == row_A);
  assert(column_C == column_B);
  
  int num_batches = 1;
  for (int i = 0; i < num_dims - 2; i++) {
    num_batches *= input_tensors[0].dim[i];
  }
  
  // Ascend AI Core block size (multiple of 16 for Cube Unit)
  int const num_threads_per_blk = 256;  // 16 AI Cores
  int num_blocks =
      (row_C * column_C + num_threads_per_blk - 1) / num_threads_per_blk;
  
  yirage::kernel::DeviceMemoryManager *dmm =
      yirage::kernel::DeviceMemoryManager::get_instance();
  
#ifdef __ASCEND__
  // Use NPU dmm->gpu_id for computing fingerprint
  checkACL(aclrtSetDevice(dmm->gpu_id));
#endif

  for (int gpu_id = 0; gpu_id < kgraph->gpu_dim.x; gpu_id++) {
    yirage::type::FPType *A_fp_ptr = reinterpret_cast<yirage::type::FPType *>(
        dmm->fp_base_ptr[gpu_id] + input_tensors[0].fp_offset);
    yirage::type::FPType *B_fp_ptr = reinterpret_cast<yirage::type::FPType *>(
        dmm->fp_base_ptr[gpu_id] + input_tensors[1].fp_offset);
    yirage::type::FPType *C_fp_ptr = reinterpret_cast<yirage::type::FPType *>(
        dmm->fp_base_ptr[gpu_id] + output_tensors[0].fp_offset);
    
#ifdef __ASCEND__
    dim3 grid(num_blocks);
    dim3 block(num_threads_per_blk);
    compute_matmul_fingerprint_ascend<<<grid, block>>>(
        A_fp_ptr, B_fp_ptr, C_fp_ptr, num_batches, row_C, column_C, row_B);
    checkACL(aclrtSynchronizeDevice());
#else
    // CPU fallback for verification without hardware
    for (int b = 0; b < num_batches; b++) {
      for (int i = 0; i < row_C; i++) {
        for (int j = 0; j < column_C; j++) {
          FPType result = 0;
          for (int p = 0; p < row_B; p++) {
            FPType x = A_fp_ptr[b * row_C * row_B + i * row_B + p];
            FPType y = B_fp_ptr[b * row_B * column_C + p * column_C + j];
            result = compute_add_fingerprint(result, compute_mul_fingerprint(x, y));
          }
          C_fp_ptr[b * row_C * column_C + i * column_C + j] = result;
        }
      }
    }
#endif
  }
  return true;
}

#endif // YIRAGE_FINGERPRINT_USE_ASCEND

} // namespace kernel
} // namespace yirage

