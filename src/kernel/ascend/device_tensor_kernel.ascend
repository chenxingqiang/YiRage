/* Copyright 2025 Chen Xingqiang (YiRage Project)
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * This file is part of YiRage (Yi Revolutionary AGile Engine)
 * 
 * Ascend Device Tensor Kernel
 * 
 * Device tensor operations for Huawei Ascend NPU backend.
 */

#include "yirage/kernel/device_memory_manager.h"
#include "yirage/kernel/device_tensor.h"
#include "yirage/utils/ascend_helper.h"
#include "yirage/utils/fingerprint_functions.h"
#include <cassert>
#include <cstring>

#ifdef __ASCEND__
#include "acl/acl.h"
#endif

namespace yirage {
namespace kernel {

using namespace yirage::type;
using namespace yirage::config;

#ifdef YIRAGE_FINGERPRINT_USE_ASCEND

/**
 * @brief Copy tensor fingerprint kernel for Ascend
 */
#ifdef __ASCEND__
__global__
#endif
void copy_tensor_fingerprint_ascend(yirage::type::FPType *src_ptr,
                                    yirage::type::FPType *dst_ptr,
                                    int num_elements) {
#ifdef __ASCEND__
  int tid = threadIdx.x + blockIdx.x * blockDim.x;
  int stride = blockDim.x * gridDim.x;
#else
  int tid = 0;
  int stride = 1;
#endif
  
  for (int i = tid; i < num_elements; i += stride) {
    dst_ptr[i] = src_ptr[i];
  }
}

void DTensor::copy_fingerprint_from(DTensor const &src) {
  assert(num_elements() == src.num_elements());
  
  yirage::kernel::DeviceMemoryManager *dmm =
      yirage::kernel::DeviceMemoryManager::get_instance();
  
  int num_elems = num_elements();
  
  int const num_threads_per_blk = 256;
  int num_blocks =
      (num_elems + num_threads_per_blk - 1) / num_threads_per_blk;
  
  yirage::type::FPType *src_ptr =
      reinterpret_cast<yirage::type::FPType *>(
          dmm->fp_base_ptr[0] + src.fp_offset);
  yirage::type::FPType *dst_ptr =
      reinterpret_cast<yirage::type::FPType *>(
          dmm->fp_base_ptr[0] + fp_offset);
  
#ifdef __ASCEND__
  checkACL(aclrtSetDevice(dmm->gpu_id));
  copy_tensor_fingerprint_ascend<<<num_blocks, num_threads_per_blk>>>(
      src_ptr, dst_ptr, num_elems);
  checkACL(aclrtSynchronizeDevice());
#else
  // CPU fallback
  for (int i = 0; i < num_elems; i++) {
    dst_ptr[i] = src_ptr[i];
  }
#endif
}

FPType DTensor::get_fingerprint() const {
  yirage::kernel::DeviceMemoryManager *dmm =
      yirage::kernel::DeviceMemoryManager::get_instance();
  
  FPType result = 0;
  int num_elems = num_elements();
  
  yirage::type::FPType *fp_ptr =
      reinterpret_cast<yirage::type::FPType *>(
          dmm->fp_base_ptr[0] + fp_offset);
  
#ifdef __ASCEND__
  // Copy to host for reduction
  std::vector<FPType> host_data(num_elems);
  checkACL(aclrtMemcpy(host_data.data(), num_elems * sizeof(FPType),
                       fp_ptr, num_elems * sizeof(FPType),
                       ACL_MEMCPY_DEVICE_TO_HOST));
  
  for (int i = 0; i < num_elems; i++) {
    result = utils::compute_add_fingerprint(result, host_data[i]);
  }
#else
  for (int i = 0; i < num_elems; i++) {
    result = utils::compute_add_fingerprint(result, fp_ptr[i]);
  }
#endif
  
  return result;
}

#endif // YIRAGE_FINGERPRINT_USE_ASCEND

} // namespace kernel
} // namespace yirage

